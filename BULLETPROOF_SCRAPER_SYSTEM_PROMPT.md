# Bulletproof Web Scraper System: QA-Integrated with AI Augmentation

## Current Issues Analysis

From your dashboard, I can see critical problems:
- **106 failures** on DC Fray scraper
- **Display errors** showing `{"event_container": "", "title": ""}` instead of readable intervals
- **0 events scraped** across all scrapers despite being "Active"
- **Inconsistent last run times** (8h, 12h, 14h ago)

## Developer Implementation Prompt

### ðŸŽ¯ **Objective**
Transform the basic web scraper into a production-ready, self-healing system that:
1. **Prevents failures** through robust error handling
2. **Self-diagnoses issues** with AI-powered anomaly detection  
3. **Auto-adapts** to website changes using intelligent selectors
4. **Maintains 99%+ uptime** with comprehensive QA integration

---

## Phase 1: Foundational Code Improvements

### 1.1 Robust HTTP Request Handling
```python
class BulletproofRequester:
    def __init__(self):
        self.session = requests.Session()
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
        ]
        self.proxy_pool = self._load_proxy_pool()
    
    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=2, max=60),
        retry=retry_if_exception_type((requests.RequestException, ConnectionError))
    )
    def safe_request(self, url: str, **kwargs) -> Optional[requests.Response]:
        """Bulletproof HTTP request with retries and rotation"""
        try:
            headers = {
                'User-Agent': random.choice(self.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Cache-Control': 'max-age=0'
            }
            
            # Random delay to avoid rate limiting
            time.sleep(random.uniform(1, 3))
            
            response = self.session.get(
                url, 
                headers=headers, 
                timeout=30,
                proxies=self._get_random_proxy(),
                **kwargs
            )
            
            # Validate response
            if response.status_code == 429:  # Rate limited
                time.sleep(60)  # Wait 1 minute
                raise requests.RequestException("Rate limited")
            
            response.raise_for_status()
            return response
            
        except Exception as e:
            logger.error(f"Request failed for {url}: {str(e)}")
            raise
    
    def _get_random_proxy(self) -> Dict:
        """Return random proxy from pool if available"""
        if self.proxy_pool:
            proxy = random.choice(self.proxy_pool)
            return {'http': proxy, 'https': proxy}
        return {}
```

### 1.2 Comprehensive Error Handling & Logging
```python
import structlog
from dataclasses import dataclass
from typing import Dict, List, Optional

@dataclass
class ScrapingResult:
    """Structured result object for all scraping operations"""
    success: bool
    events_found: int
    events_added: int
    errors: List[str]
    warnings: List[str]
    execution_time: float
    confidence_scores: List[int]
    metadata: Dict

class ScraperLogger:
    def __init__(self):
        self.logger = structlog.get_logger()
    
    def log_scraping_attempt(self, scraper_id: int, url: str):
        """Log start of scraping operation"""
        self.logger.info(
            "scraping_started",
            scraper_id=scraper_id,
            url=url,
            timestamp=datetime.utcnow().isoformat()
        )
    
    def log_scraping_result(self, scraper_id: int, result: ScrapingResult):
        """Log detailed scraping results"""
        log_level = "info" if result.success else "error"
        getattr(self.logger, log_level)(
            "scraping_completed",
            scraper_id=scraper_id,
            **asdict(result)
        )
    
    def log_anomaly(self, scraper_id: int, anomaly_type: str, details: Dict):
        """Log data anomalies detected by AI"""
        self.logger.warning(
            "anomaly_detected",
            scraper_id=scraper_id,
            anomaly_type=anomaly_type,
            details=details
        )
```

---

## Phase 2: QA Framework Implementation

### 2.1 Data Validation Schema
```python
from pydantic import BaseModel, validator, Field
from datetime import datetime
from typing import Optional, List

class EventSchema(BaseModel):
    """Strict schema for event data validation"""
    title: str = Field(..., min_length=3, max_length=200)
    description: Optional[str] = Field(None, max_length=2000)
    start_date: str = Field(..., description="ISO format or parseable date string")
    end_date: Optional[str] = None
    location: Optional[str] = Field(None, max_length=200)
    price_info: Optional[str] = Field(None, max_length=100)
    url: Optional[str] = None
    image: Optional[str] = None
    category: str = Field(default="events")
    confidence_score: int = Field(..., ge=0, le=100)
    source_url: str = Field(..., description="Original scraping URL")
    
    @validator('start_date')
    def validate_start_date(cls, v):
        """Ensure date is parseable and in future"""
        try:
            parsed_date = dateutil.parser.parse(v)
            if parsed_date.date() < datetime.now().date():
                raise ValueError("Event date cannot be in the past")
            return v
        except Exception:
            raise ValueError("Invalid date format")
    
    @validator('title')
    def validate_title_quality(cls, v):
        """Ensure title doesn't contain spam indicators"""
        spam_keywords = ['viagra', 'casino', 'bitcoin', 'mlm', 'free money']
        if any(keyword in v.lower() for keyword in spam_keywords):
            raise ValueError("Title contains spam indicators")
        return v
    
    @validator('confidence_score')
    def validate_confidence_threshold(cls, v):
        """Ensure minimum confidence for production"""
        if v < 60:
            raise ValueError("Confidence score too low for production")
        return v

class QAValidator:
    """Production-ready data validation system"""
    
    def __init__(self):
        self.schema = EventSchema
        self.anomaly_detector = AnomalyDetector()
    
    def validate_batch(self, events: List[Dict]) -> Tuple[List[Dict], List[str]]:
        """Validate entire batch of scraped events"""
        valid_events = []
        errors = []
        
        for i, event_data in enumerate(events):
            try:
                # Schema validation
                validated_event = self.schema(**event_data)
                
                # AI anomaly detection
                anomalies = self.anomaly_detector.detect_anomalies(validated_event.dict())
                if anomalies:
                    errors.append(f"Event {i}: Anomalies detected: {anomalies}")
                    continue
                
                valid_events.append(validated_event.dict())
                
            except Exception as e:
                errors.append(f"Event {i}: Validation failed: {str(e)}")
        
        return valid_events, errors
```

### 2.2 Regression Testing Framework
```python
import pytest
from pathlib import Path
import json

class ScraperRegressionTester:
    """Automated regression testing for scraper reliability"""
    
    def __init__(self, test_fixtures_dir: str = "test_fixtures"):
        self.fixtures_dir = Path(test_fixtures_dir)
        self.fixtures_dir.mkdir(exist_ok=True)
    
    def capture_golden_sample(self, scraper_id: int, html_content: str, expected_events: List[Dict]):
        """Capture HTML and expected output for regression testing"""
        fixture_file = self.fixtures_dir / f"scraper_{scraper_id}_golden.json"
        
        fixture_data = {
            'scraper_id': scraper_id,
            'html_content': html_content,
            'expected_events': expected_events,
            'captured_at': datetime.utcnow().isoformat(),
            'version': '1.0'
        }
        
        with open(fixture_file, 'w') as f:
            json.dump(fixture_data, f, indent=2)
    
    def run_regression_tests(self, scraper: EnhancedWebScraper) -> Dict[int, bool]:
        """Run regression tests against all golden samples"""
        results = {}
        
        for fixture_file in self.fixtures_dir.glob("scraper_*_golden.json"):
            with open(fixture_file, 'r') as f:
                fixture_data = json.load(f)
            
            scraper_id = fixture_data['scraper_id']
            html_content = fixture_data['html_content']
            expected_events = fixture_data['expected_events']
            
            # Run scraper on golden HTML
            soup = BeautifulSoup(html_content, 'html.parser')
            actual_events = scraper._extract_events_from_soup(soup, "test_url")
            
            # Compare results (simplified comparison)
            passed = self._compare_event_lists(expected_events, actual_events)
            results[scraper_id] = passed
            
            if not passed:
                logger.error(f"Regression test failed for scraper {scraper_id}")
        
        return results
    
    def _compare_event_lists(self, expected: List[Dict], actual: List[Dict]) -> bool:
        """Compare event lists for regression testing"""
        if len(expected) != len(actual):
            return False
        
        # Compare key fields
        for exp, act in zip(expected, actual):
            if (exp.get('title') != act.get('title') or 
                exp.get('start_date') != act.get('start_date')):
                return False
        
        return True
```

### 2.3 Real-time Monitoring System
```python
from dataclasses import dataclass
from typing import Dict, List
import smtplib
from email.mime.text import MimeText

@dataclass
class HealthMetrics:
    scraper_id: int
    success_rate: float
    avg_events_per_run: float
    avg_confidence_score: float
    consecutive_failures: int
    last_successful_run: datetime
    
    @property
    def health_score(self) -> int:
        """Calculate overall health score (0-100)"""
        score = 100
        
        # Penalize low success rate
        if self.success_rate < 0.9:
            score -= (0.9 - self.success_rate) * 200
        
        # Penalize consecutive failures
        score -= min(self.consecutive_failures * 10, 50)
        
        # Penalize stale data
        if self.last_successful_run:
            hours_since_success = (datetime.utcnow() - self.last_successful_run).total_seconds() / 3600
            if hours_since_success > 24:
                score -= min(hours_since_success, 30)
        
        return max(0, int(score))

class MonitoringSystem:
    """Real-time monitoring and alerting for scrapers"""
    
    def __init__(self, alert_threshold: int = 70):
        self.alert_threshold = alert_threshold
        self.metrics_history = {}
    
    def calculate_health_metrics(self, scraper_id: int) -> HealthMetrics:
        """Calculate current health metrics for a scraper"""
        conn = sqlite3.connect('calendar.db')
        cursor = conn.cursor()
        
        # Get recent run history (last 30 days)
        cursor.execute('''
            SELECT success, events_found, confidence_score, created_at
            FROM scraper_runs 
            WHERE scraper_id = ? AND created_at > datetime('now', '-30 days')
            ORDER BY created_at DESC
        ''', (scraper_id,))
        
        runs = cursor.fetchall()
        
        if not runs:
            return HealthMetrics(
                scraper_id=scraper_id,
                success_rate=0.0,
                avg_events_per_run=0.0,
                avg_confidence_score=0.0,
                consecutive_failures=999,
                last_successful_run=None
            )
        
        # Calculate metrics
        total_runs = len(runs)
        successful_runs = [r for r in runs if r[0]]  # success = True
        success_rate = len(successful_runs) / total_runs
        
        avg_events = sum(r[1] for r in successful_runs) / len(successful_runs) if successful_runs else 0
        avg_confidence = sum(r[2] for r in successful_runs) / len(successful_runs) if successful_runs else 0
        
        # Count consecutive failures from most recent
        consecutive_failures = 0
        for run in runs:
            if run[0]:  # If successful, break
                break
            consecutive_failures += 1
        
        last_success = successful_runs[0][3] if successful_runs else None
        
        return HealthMetrics(
            scraper_id=scraper_id,
            success_rate=success_rate,
            avg_events_per_run=avg_events,
            avg_confidence_score=avg_confidence,
            consecutive_failures=consecutive_failures,
            last_successful_run=datetime.fromisoformat(last_success) if last_success else None
        )
    
    def check_and_alert(self, scraper_id: int):
        """Check health and send alerts if needed"""
        metrics = self.calculate_health_metrics(scraper_id)
        
        if metrics.health_score < self.alert_threshold:
            self._send_alert(metrics)
    
    def _send_alert(self, metrics: HealthMetrics):
        """Send alert notification"""
        subject = f"ðŸš¨ Scraper {metrics.scraper_id} Health Alert"
        body = f"""
        Scraper Health Alert
        
        Scraper ID: {metrics.scraper_id}
        Health Score: {metrics.health_score}%
        Success Rate: {metrics.success_rate:.1%}
        Consecutive Failures: {metrics.consecutive_failures}
        Last Successful Run: {metrics.last_successful_run}
        
        Action Required: Investigate scraper configuration and target website.
        """
        
        # Send email alert (configure SMTP settings)
        self._send_email(subject, body)
        
        # Log alert
        logger.critical("health_alert_sent", scraper_id=metrics.scraper_id, health_score=metrics.health_score)
```

---

## Phase 3: AI Integration

### 3.1 Intelligent Selector Generation
```python
from transformers import pipeline
import openai
from typing import List, Dict

class AISelectorGenerator:
    """AI-powered CSS selector generation for resilient scraping"""
    
    def __init__(self):
        self.classifier = pipeline("text-classification", model="distilbert-base-uncased")
        # Configure OpenAI API key
        openai.api_key = os.getenv('OPENAI_API_KEY')
    
    def generate_resilient_selectors(self, html_content: str, target_type: str) -> List[str]:
        """Generate multiple selector options using AI analysis"""
        
        # Parse HTML structure
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Extract potential target elements
        candidates = self._find_element_candidates(soup, target_type)
        
        # Use AI to rank and generate selectors
        selectors = []
        
        for candidate in candidates[:10]:  # Limit to top 10 candidates
            # Generate multiple selector strategies
            selectors.extend([
                self._generate_css_selector(candidate),
                self._generate_xpath_selector(candidate),
                self._generate_semantic_selector(candidate, target_type)
            ])
        
        # Filter and rank selectors
        return self._rank_selectors(selectors, html_content, target_type)
    
    def _find_element_candidates(self, soup: BeautifulSoup, target_type: str) -> List:
        """Find potential elements that might contain target data"""
        
        if target_type == "title":
            return soup.find_all(['h1', 'h2', 'h3', 'h4', 'a', 'span', 'div'], 
                                string=re.compile(r'.{5,}'))  # Text with 5+ chars
        
        elif target_type == "date":
            return soup.find_all(attrs={'class': re.compile(r'date|time|when'), 
                                       'id': re.compile(r'date|time|when')})
        
        elif target_type == "location":
            return soup.find_all(attrs={'class': re.compile(r'location|venue|address|where'), 
                                       'id': re.compile(r'location|venue|address|where')})
        
        elif target_type == "price":
            return soup.find_all(string=re.compile(r'\$\d+|\d+\.\d{2}|free|cost'))
        
        return []
    
    def _generate_semantic_selector(self, element, target_type: str) -> str:
        """Use AI to generate semantic-based selectors"""
        
        # Extract context around element
        context = {
            'tag': element.name,
            'classes': element.get('class', []),
            'id': element.get('id'),
            'text': element.get_text(strip=True)[:100],
            'parent_classes': element.parent.get('class', []) if element.parent else [],
            'siblings': [sib.name for sib in element.next_siblings if hasattr(sib, 'name')][:5]
        }
        
        # Use OpenAI to generate intelligent selector
        prompt = f"""
        Generate a robust CSS selector for extracting {target_type} from this HTML element context:
        
        Element: {context}
        
        Requirements:
        - Should be resilient to minor HTML changes
        - Prefer semantic meaning over specific class names
        - Should be as specific as necessary but as general as possible
        
        Return only the CSS selector:
        """
        
        try:
            response = openai.Completion.create(
                engine="text-davinci-003",
                prompt=prompt,
                max_tokens=50,
                temperature=0.1
            )
            
            selector = response.choices[0].text.strip()
            return selector
            
        except Exception as e:
            logger.warning(f"AI selector generation failed: {e}")
            return self._generate_css_selector(element)  # Fallback
    
    def _rank_selectors(self, selectors: List[str], html_content: str, target_type: str) -> List[str]:
        """Rank selectors by reliability and specificity"""
        soup = BeautifulSoup(html_content, 'html.parser')
        ranked = []
        
        for selector in selectors:
            try:
                elements = soup.select(selector)
                if elements:
                    # Score based on number of matches and content quality
                    score = self._score_selector(elements, target_type)
                    ranked.append((selector, score))
            except Exception:
                continue  # Invalid selector
        
        # Sort by score and return top selectors
        ranked.sort(key=lambda x: x[1], reverse=True)
        return [selector for selector, score in ranked[:5]]
    
    def _score_selector(self, elements: List, target_type: str) -> float:
        """Score selector quality based on extracted content"""
        if not elements:
            return 0.0
        
        score = 0.0
        
        # Prefer selectors that find 1-10 elements (not too many, not too few)
        element_count = len(elements)
        if 1 <= element_count <= 10:
            score += 1.0
        elif element_count == 0:
            return 0.0
        else:
            score += 0.5  # Too many elements
        
        # Score based on content quality
        for element in elements[:5]:  # Check first 5 elements
            text = element.get_text(strip=True)
            
            if target_type == "title" and 5 <= len(text) <= 100:
                score += 0.5
            elif target_type == "date" and re.search(r'\d{1,2}[/-]\d{1,2}[/-]\d{2,4}|\w+\s+\d{1,2}', text):
                score += 0.5
            elif target_type == "location" and len(text) > 3:
                score += 0.3
            elif target_type == "price" and re.search(r'\$\d+|free|cost', text.lower()):
                score += 0.5
        
        return score
```

### 3.2 AI-Powered Anomaly Detection
```python
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import joblib

class AnomalyDetector:
    """AI-powered anomaly detection for scraped data quality"""
    
    def __init__(self, model_path: str = "anomaly_model.pkl"):
        self.model_path = model_path
        self.scaler = StandardScaler()
        self.model = None
        self.feature_names = [
            'title_length', 'description_length', 'has_date', 'has_location',
            'has_price', 'confidence_score', 'url_length', 'has_image'
        ]
        
        # Load or initialize model
        self._load_or_train_model()
    
    def _extract_features(self, event_data: Dict) -> np.array:
        """Extract numerical features from event data for ML model"""
        features = [
            len(event_data.get('title', '')),
            len(event_data.get('description', '')),
            1 if event_data.get('start_date') else 0,
            1 if event_data.get('location') else 0,
            1 if event_data.get('price_info') else 0,
            event_data.get('confidence_score', 0),
            len(event_data.get('url', '')),
            1 if event_data.get('image') else 0
        ]
        return np.array(features).reshape(1, -1)
    
    def detect_anomalies(self, event_data: Dict) -> List[str]:
        """Detect anomalies in scraped event data"""
        anomalies = []
        
        # Rule-based anomaly detection
        rule_anomalies = self._rule_based_detection(event_data)
        anomalies.extend(rule_anomalies)
        
        # ML-based anomaly detection
        if self.model:
            ml_anomalies = self._ml_based_detection(event_data)
            anomalies.extend(ml_anomalies)
        
        return anomalies
    
    def _rule_based_detection(self, event_data: Dict) -> List[str]:
        """Rule-based anomaly detection"""
        anomalies = []
        
        title = event_data.get('title', '')
        description = event_data.get('description', '')
        price_info = event_data.get('price_info', '')
        
        # Title anomalies
        if len(title) < 3:
            anomalies.append("Title too short")
        elif len(title) > 200:
            anomalies.append("Title suspiciously long")
        elif title.isupper() and len(title) > 20:
            anomalies.append("Title in all caps (possible spam)")
        
        # Description anomalies
        if len(description) > 5000:
            anomalies.append("Description extremely long")
        
        # Price anomalies
        if price_info:
            price_numbers = re.findall(r'\$(\d+(?:\.\d{2})?)', price_info)
            if price_numbers:
                max_price = max(float(p) for p in price_numbers)
                if max_price > 10000:
                    anomalies.append("Price suspiciously high")
                elif max_price < 0.01:
                    anomalies.append("Price suspiciously low")
        
        # Date anomalies
        start_date = event_data.get('start_date')
        if start_date:
            try:
                event_date = dateutil.parser.parse(start_date)
                days_in_future = (event_date.date() - datetime.now().date()).days
                
                if days_in_future > 365:
                    anomalies.append("Event date more than 1 year in future")
                elif days_in_future < -1:
                    anomalies.append("Event date in the past")
            except:
                anomalies.append("Invalid date format")
        
        return anomalies
    
    def _ml_based_detection(self, event_data: Dict) -> List[str]:
        """ML-based anomaly detection using Isolation Forest"""
        anomalies = []
        
        try:
            features = self._extract_features(event_data)
            features_scaled = self.scaler.transform(features)
            
            # Predict anomaly (-1 = anomaly, 1 = normal)
            prediction = self.model.predict(features_scaled)[0]
            
            if prediction == -1:
                # Get anomaly score for more details
                score = self.model.decision_function(features_scaled)[0]
                anomalies.append(f"ML model detected anomaly (score: {score:.3f})")
        
        except Exception as e:
            logger.warning(f"ML anomaly detection failed: {e}")
        
        return anomalies
    
    def train_model(self, training_data: List[Dict]):
        """Train anomaly detection model on historical good data"""
        if len(training_data) < 100:
            logger.warning("Insufficient training data for ML model")
            return
        
        # Extract features from training data
        features_list = []
        for event in training_data:
            features = self._extract_features(event)
            features_list.append(features.flatten())
        
        X = np.array(features_list)
        
        # Fit scaler and model
        X_scaled = self.scaler.fit_transform(X)
        
        self.model = IsolationForest(
            contamination=0.1,  # Expect 10% anomalies
            random_state=42,
            n_estimators=100
        )
        
        self.model.fit(X_scaled)
        
        # Save model
        joblib.dump({
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': self.feature_names
        }, self.model_path)
        
        logger.info(f"Anomaly detection model trained on {len(training_data)} samples")
    
    def _load_or_train_model(self):
        """Load existing model or train new one"""
        try:
            if os.path.exists(self.model_path):
                saved_data = joblib.load(self.model_path)
                self.model = saved_data['model']
                self.scaler = saved_data['scaler']
                self.feature_names = saved_data['feature_names']
                logger.info("Loaded existing anomaly detection model")
            else:
                logger.info("No existing model found. Will train when data is available.")
        except Exception as e:
            logger.error(f"Failed to load anomaly model: {e}")
```

---

## Phase 4: Integration & Production Deployment

### 4.1 Updated Scraper Class with All Components
```python
class BulletproofWebScraper(EnhancedWebScraper):
    """Production-ready scraper with QA and AI integration"""
    
    def __init__(self):
        super().__init__()
        self.requester = BulletproofRequester()
        self.qa_validator = QAValidator()
        self.regression_tester = ScraperRegressionTester()
        self.monitor = MonitoringSystem()
        self.ai_selector = AISelectorGenerator()
        self.anomaly_detector = AnomalyDetector()
        self.logger = ScraperLogger()
    
    def scrape_with_qa(self, scraper_id: int, url: str, custom_selectors: Dict = None) -> ScrapingResult:
        """Main scraping method with full QA integration"""
        start_time = time.time()
        self.logger.log_scraping_attempt(scraper_id, url)
        
        result = ScrapingResult(
            success=False,
            events_found=0,
            events_added=0,
            errors=[],
            warnings=[],
            execution_time=0,
            confidence_scores=[],
            metadata={}
        )
        
        try:
            # Step 1: Pre-scraping health check
            if not self._pre_scraping_health_check(scraper_id):
                result.errors.append("Pre-scraping health check failed")
                return result
            
            # Step 2: Fetch page with bulletproof requesting
            response = self.requester.safe_request(url)
            if not response:
                result.errors.append("Failed to fetch page")
                return result
            
            # Step 3: Extract events with AI-enhanced selectors
            events = self._extract_with_ai_selectors(response.text, url, custom_selectors)
            result.events_found = len(events)
            
            # Step 4: Validate with QA system
            valid_events, validation_errors = self.qa_validator.validate_batch(events)
            result.errors.extend(validation_errors)
            
            # Step 5: Store validated events
            events_added = self._store_validated_events(scraper_id, valid_events)
            result.events_added = events_added
            result.confidence_scores = [e.get('confidence_score', 0) for e in valid_events]
            
            # Step 6: Update scraper health metrics
            self._update_health_metrics(scraper_id, result)
            
            result.success = events_added > 0
            
        except Exception as e:
            result.errors.append(f"Scraping failed: {str(e)}")
            logger.exception(f"Scraping error for {url}")
        
        finally:
            result.execution_time = time.time() - start_time
            self.logger.log_scraping_result(scraper_id, result)
            
            # Post-scraping monitoring
            self.monitor.check_and_alert(scraper_id)
        
        return result
    
    def _extract_with_ai_selectors(self, html_content: str, url: str, custom_selectors: Dict) -> List[Dict]:
        """Extract events using AI-generated selectors as fallback"""
        events = []
        
        # Try custom selectors first
        if custom_selectors:
            events = super().scrape_events(url, custom_selectors)
        
        # If no events found, use AI to generate new selectors
        if not events:
            logger.info("No events found with custom selectors, trying AI-generated selectors")
            
            ai_selectors = {
                'title': self.ai_selector.generate_resilient_selectors(html_content, 'title'),
                'date': self.ai_selector.generate_resilient_selectors(html_content, 'date'),
                'location': self.ai_selector.generate_resilient_selectors(html_content, 'location')
            }
            
            # Try each AI-generated selector combination
            for title_sel in ai_selectors['title'][:3]:
                for date_sel in ai_selectors['date'][:3]:
                    test_selectors = {
                        'title': title_sel,
                        'date': date_sel,
                        'location': ai_selectors['location'][0] if ai_selectors['location'] else ''
                    }
                    
                    test_events = super().scrape_events(url, test_selectors)
                    if test_events and len(test_events) > len(events):
                        events = test_events
        
        return events
    
    def _pre_scraping_health_check(self, scraper_id: int) -> bool:
        """Check if scraper is healthy enough to run"""
        metrics = self.monitor.calculate_health_metrics(scraper_id)
        
        # Don't run if health score is critically low
        if metrics.health_score < 30:
            logger.warning(f"Scraper {scraper_id} health too low ({metrics.health_score}%), skipping run")
            return False
        
        # Don't run if too many consecutive failures
        if metrics.consecutive_failures > 10:
            logger.warning(f"Scraper {scraper_id} has {metrics.consecutive_failures} consecutive failures, skipping run")
            return False
        
        return True
    
    def auto_heal_scraper(self, scraper_id: int) -> bool:
        """Attempt to automatically fix a failing scraper"""
        logger.info(f"Attempting auto-heal for scraper {scraper_id}")
        
        # Get scraper configuration
        conn = sqlite3.connect('calendar.db')
        cursor = conn.cursor()
        cursor.execute('SELECT url, selector_config FROM web_scrapers WHERE id = ?', (scraper_id,))
        row = cursor.fetchone()
        
        if not row:
            return False
        
        url, selector_config = row
        
        try:
            # Fetch current page
            response = self.requester.safe_request(url)
            if not response:
                return False
            
            # Generate new selectors using AI
            new_selectors = {}
            for field_type in ['title', 'date', 'location', 'description']:
                selectors = self.ai_selector.generate_resilient_selectors(response.text, field_type)
                if selectors:
                    new_selectors[field_type] = selectors[0]  # Use best selector
            
            # Test new selectors
            test_events = super().scrape_events(url, new_selectors)
            
            if test_events and len(test_events) > 0:
                # Update scraper configuration
                cursor.execute(
                    'UPDATE web_scrapers SET selector_config = ?, consecutive_failures = 0 WHERE id = ?',
                    (json.dumps(new_selectors), scraper_id)
                )
                conn.commit()
                
                logger.info(f"Auto-heal successful for scraper {scraper_id}, found {len(test_events)} events")
                return True
        
        except Exception as e:
            logger.error(f"Auto-heal failed for scraper {scraper_id}: {e}")
        
        finally:
            conn.close()
        
        return False
```

### 4.2 Production Scheduler with Health Monitoring
```python
import schedule
import threading
from concurrent.futures import ThreadPoolExecutor

class ProductionScheduler:
    """Production scheduler with health monitoring and auto-healing"""
    
    def __init__(self, max_workers: int = 5):
        self.scraper = BulletproofWebScraper()
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.running = False
    
    def start_scheduler(self):
        """Start the production scheduler"""
        logger.info("Starting production scheduler with health monitoring")
        
        # Schedule regular scraping
        schedule.every(30).minutes.do(self._run_all_scrapers)
        
        # Schedule health checks
        schedule.every(1).hours.do(self._health_check_all_scrapers)
        
        # Schedule auto-healing
        schedule.every(6).hours.do(self._auto_heal_failing_scrapers)
        
        # Schedule regression tests
        schedule.every(1).days.do(self._run_regression_tests)
        
        self.running = True
        
        while self.running:
            schedule.run_pending()
            time.sleep(60)  # Check every minute
    
    def _run_all_scrapers(self):
        """Run all active scrapers in parallel"""
        conn = sqlite3.connect('calendar.db')
        cursor = conn.cursor()
        cursor.execute('SELECT id, url, selector_config FROM web_scrapers WHERE is_active = 1')
        scrapers = cursor.fetchall()
        conn.close()
        
        # Submit scraping jobs to thread pool
        futures = []
        for scraper_id, url, selector_config in scrapers:
            future = self.executor.submit(
                self.scraper.scrape_with_qa,
                scraper_id,
                url,
                json.loads(selector_config) if selector_config else None
            )
            futures.append((scraper_id, future))
        
        # Wait for completion and log results
        for scraper_id, future in futures:
            try:
                result = future.result(timeout=300)  # 5 minute timeout
                logger.info(f"Scraper {scraper_id} completed: {result.events_added} events added")
            except Exception as e:
                logger.error(f"Scraper {scraper_id} failed: {e}")
    
    def _health_check_all_scrapers(self):
        """Perform health checks on all scrapers"""
        conn = sqlite3.connect('calendar.db')
        cursor = conn.cursor()
        cursor.execute('SELECT id FROM web_scrapers WHERE is_active = 1')
        scraper_ids = [row[0] for row in cursor.fetchall()]
        conn.close()
        
        for scraper_id in scraper_ids:
            self.scraper.monitor.check_and_alert(scraper_id)
    
    def _auto_heal_failing_scrapers(self):
        """Attempt to auto-heal failing scrapers"""
        conn = sqlite3.connect('calendar.db')
        cursor = conn.cursor()
        cursor.execute('''
            SELECT id FROM web_scrapers 
            WHERE is_active = 1 AND consecutive_failures > 5
        ''')
        failing_scrapers = [row[0] for row in cursor.fetchall()]
        conn.close()
        
        for scraper_id in failing_scrapers:
            self.scraper.auto_heal_scraper(scraper_id)
    
    def _run_regression_tests(self):
        """Run daily regression tests"""
        results = self.scraper.regression_tester.run_regression_tests(self.scraper)
        
        failed_tests = [scraper_id for scraper_id, passed in results.items() if not passed]
        
        if failed_tests:
            logger.error(f"Regression tests failed for scrapers: {failed_tests}")
            # Could trigger auto-healing or alerts here
        else:
            logger.info("All regression tests passed")

# Start the production system
if __name__ == "__main__":
    scheduler = ProductionScheduler()
    scheduler.start_scheduler()
```

---

## Expected Outcomes

### Immediate Improvements (Week 1)
- **Zero 500 errors** - Robust error handling prevents crashes
- **Proper display** - Fix `{"event_container"...}` display issues
- **Consistent runs** - All scrapers run on schedule

### Short-term Gains (Month 1)
- **95%+ success rate** - Bulletproof requesting and retries
- **Smart failure recovery** - Auto-healing for website changes
- **Real-time alerts** - Immediate notification of issues

### Long-term Benefits (Month 3+)
- **Self-maintaining system** - AI adapts to website changes
- **Predictive maintenance** - Anomaly detection prevents issues
- **99%+ uptime** - Production-grade reliability

This bulletproof system transforms your web scraper from a basic tool into an enterprise-grade, self-healing solution that maintains high performance and reliability.
